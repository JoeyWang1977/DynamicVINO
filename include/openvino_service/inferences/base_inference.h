//
// Created by chris on 18-8-9.
//

#ifndef OPENVINO_PIPELINE_LIB_BASE_INFERENCE_H
#define OPENVINO_PIPELINE_LIB_BASE_INFERENCE_H

#include <memory>

#include "opencv2/opencv.hpp"
#include "inference_engine.hpp"

#include "openvino_service/engines/engine.h"
#include "openvino_service/data_struct.h"
#include "openvino_service/outputs/output.h"
#include "openvino_service/slog.hpp"

// utils
template<typename T>
void
matU8ToBlob(const cv::Mat &orig_image,
            InferenceEngine::Blob::Ptr &blob,
            float scale_factor = 1.0,
            int batch_index = 0) {
  InferenceEngine::SizeVector blob_size = blob->getTensorDesc().getDims();
  const size_t width = blob_size[3];
  const size_t height = blob_size[2];
  const size_t channels = blob_size[1];
  T *blob_data = blob->buffer().as<T *>();

  cv::Mat resized_image(orig_image);
  if (width != orig_image.size().width ||
      height != orig_image.size().height) {
    cv::resize(orig_image, resized_image, cv::Size(width, height));
  }
  int batchOffset = batch_index * width * height * channels;

  for (size_t c = 0; c < channels; c++) {
    for (size_t h = 0; h < height; h++) {
      for (size_t w = 0; w < width; w++) {
        blob_data[batchOffset + c * width * height + h * width + w] =
            resized_image.at<cv::Vec3b>(h, w)[c] * scale_factor;
      }
    }
  }
}

namespace openvino_service {
/**
 * @class BaseInference
 * @brief base class for network inference.
 */
class BaseInference {
 public:
  explicit BaseInference();
  virtual ~BaseInference();
  /**
   * @brief load the Engine instance that contains the request for
   * running netwrok on target calculation device.
   */
  void loadEngine(std::shared_ptr<NetworkEngine> engine);
  /**
   * @brief Get the loaded Engine instance.
   */
  inline const std::shared_ptr<NetworkEngine> getEngine() const {
    return engine_;
  }
  /**
   * @brief Get the number of enqueued frames to be infered.
   */
  inline const int getEnqueuedNum() const { return enqueued_frames; }
  /**
   * @brief Enqueue a frame to this class.
   * The frame will be buffered but not infered yet.
   * @param[in] frame The frame to be enqueued.
   * @param[in] input_frame_loc The location of the enqueued frame with respect
   * to the frame generated by the input device.
   * @return whether this operation is successful.
   */
  virtual bool
  enqueue(const cv::Mat &frame, const cv::Rect &input_frame_loc) = 0;
  /**
   * @brief Start inference for all buffered frames.
   * @return whether this operation is successful.
   */
  virtual bool submitRequest();
  /**
   * @brief This function will fetch the results of the previous inference and
   * stores the results in a result buffer array. All buffered frames will be
   * cleared.
   * @return whether the Inference object fetches a result this time
   */
  virtual bool fetchResults();
  /**
   * @brief Accepts an Output instance for result process. This function is
   * used for visitor pattern.
   */
  virtual void accepts(std::shared_ptr<BaseOutput> output_visitor) = 0;
  /**
   * @brief Get the length of the buffer result array.
   */
  virtual const int getResultsLength() const = 0;
  /**
   * @brief Get the location of result with respect
   * to the frame generated by the input device.
   * @param[in] idx The index of the result.
   */
  virtual const InferenceResult::Result
  getLocationResult(int idx) const = 0;
  /**
   * @brief Get the name of the Inference instance.
   */
  virtual const std::string getName() const = 0;

 protected:
  /**
    * @brief Enqueue the fram into the input blob of the target calculation
    * device. Check OpenVINO document for detailed information.
    */
  template<typename T>
  bool enqueue(const cv::Mat &frame, const cv::Rect &,
               float scale_factor, int batch_index,
               const std::string & input_name) {
    if (enqueued_frames == max_batch_size_) {
      slog::warn << "Number of " << getName() <<
                 "input more than maximum("
                 << max_batch_size_
                 << ") processed by inference" << slog::endl;
      return false;
    }
    InferenceEngine::Blob::Ptr input_blob
        = engine_->getRequest()->GetBlob(input_name);
    matU8ToBlob<T>(frame, input_blob, scale_factor, batch_index);
    enqueued_frames += 1;
    return true;
  }
  /**
   * @brief Set the max batch size for one inference.
   */
  inline void setMaxBatchSize(int max_batch_size) {
    max_batch_size_ = max_batch_size;
  }

 private:
  std::shared_ptr<NetworkEngine> engine_;
  int max_batch_size_ = 1;
  int enqueued_frames = 0;
  bool results_fetched_ = false;
};
}

#endif //OPENVINO_PIPELINE_LIB_BASE_INFERENCE_H
